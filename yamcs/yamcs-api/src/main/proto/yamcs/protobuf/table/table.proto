syntax = "proto2";

package yamcs.protobuf.table;

import "yamcs/api/annotations.proto";
import "yamcs/protobuf/yamcs.proto";
import "yamcs/protobuf/yamcsManagement/yamcsManagement.proto";
import "google/protobuf/timestamp.proto";

option java_package = "org.yamcs.protobuf";

// Service for reading and writing to Yamcs tables and streams
service TableApi {

  // Execute SQL
  rpc ExecuteSql(ExecuteSqlRequest) returns (ResultSet) {
    option (yamcs.api.route) = {
      post: "/api/archive/{instance}:executeSql"
      body: "*"
    };
  }

  // Execute streaming SQL
  rpc ExecuteStreamingSql(ExecuteSqlRequest) returns (stream ResultSet) {
    option (yamcs.api.route) = {
      post: "/api/archive/{instance}:executeStreamingSql"
      body: "*"
    };
  }

  // List streams
  //
  // Note that this will only list the fixed columns of the stream.
  // Tuples may always have extra columns.
  rpc ListStreams(ListStreamsRequest) returns (ListStreamsResponse) {
    option (yamcs.api.route) = {
      get: "/api/archive/{instance}/streams"
    };
  }
  
  // Receive updates on stream stats
  rpc SubscribeStreamStatistics(SubscribeStreamStatisticsRequest) returns (stream yamcsManagement.StreamEvent) {
    option (yamcs.api.websocket) = {
      topic: "stream-stats"
    };
  }
  
  // Get a stream
  rpc GetStream(GetStreamRequest) returns (StreamInfo) {
    option (yamcs.api.route) = {
      get: "/api/archive/{instance}/streams/{name}"
    };
  }
  
  // Receive stream updates
  rpc SubscribeStream(SubscribeStreamRequest) returns (stream StreamData) {
    option (yamcs.api.websocket) = {
      topic: "stream"
    };
  }
  
  // List tables
  //
  // The response will only include fixed columns of the table. Tuples may always
  // add extra value columns.
  rpc ListTables(ListTablesRequest) returns (ListTablesResponse) {
    option (yamcs.api.route) = {
      get: "/api/archive/{instance}/tables"
    };
  }
  
  // Get a table
  rpc GetTable(GetTableRequest) returns (TableInfo) {
    option (yamcs.api.route) = {
      get: "/api/archive/{instance}/tables/{name}"
    };
  }
  
  // Get table data
  rpc GetTableData(GetTableDataRequest) returns (TableData) {
    option (yamcs.api.route) = {
      get: "/api/archive/{instance}/tables/{name}/data"
    };
  }
  
  // Streams back the contents of all rows in key order
  //
  // The ``ColumnInfo`` message assigns an integer ``id`` for each column
  // and the ``id`` is present in each cell belonging to that column (this
  // is done in order to avoid sending the ``ColumnInfo`` with each ``Cell``).
  // The column id starts from 0 and are incremented with each new column found.
  // The ids are only valid during one single dump.
  // The dumped data does not contain information on any table characteristics
  // such as (primary) key, partitioning or other storage options.
  rpc ReadRows(ReadRowsRequest) returns (stream Row) {
    option (yamcs.api.route) = {
      post: "/api/archive/{instance}/tables/{table}:readRows"
      body: "*"
    };
  }
  
  // Imports a stream of rows
  //
  // The table has to exist in order to load data into it.
  //
  // As soon as the server detects an error with one of the written
  // rows, it will forcefully close the connection and send back an
  // early error message. The client should stop streaming and handle
  // the error.
  //
  // Note that the erratic condition causes the connection to be closed
  // even if the ``Keep-Alive`` request header was enabled.
  //
  // The error response is of type ``ExceptionMessage`` and contains a
  // detail message of type ``WriteRowsExceptionDetail`` that provides
  // the number of rows that were successfully written by the client.
  // The client can use this information to link the error message to a
  // row (i.e. the bad row is at position ``count + 1`` of the stream).
  //
  // One possible error could be that the table has defined a (primary)
  // key and one of the loaded rows contains no value for one of the
  // columns of the key.
  //
  // The table load will overwrite any data existing in the table with the 
  // same key as the imported row.
  //
  // The table load will not update the historgrams so a histogram rebuild
  // is required after the load.  
  rpc WriteRows(stream WriteRowsRequest) returns (WriteRowsResponse) {
    option (yamcs.api.route) = {
      post: "/api/archive/{instance}/tables/{table}:writeRows"
      body: "row"
    };
  }
  
  // Rebulds histograms - this is required after a table load.
  //
  // Turrently the time interval passed in the request will be used to select the 
  // partitions which will be rebuild - any partition overlapping with the interval will be rebuilt.
  // If the table is not partitioned by time,  the histogram for the entire table will be rebuild.
  rpc RebuildHistogram(RebuildHistogramRequest) returns (RebuildHistogramResponse) {
    option (yamcs.api.route) = {
      post: "/api/archive/{instance}/tables/{table}:rebuildHistogram"
      body: "*"
    };
  }
}

message Row {
  message ColumnInfo {
    optional uint32 id = 1;
    optional string name = 2;  
    optional string type = 3;
    
    // The name of the class implementing the proto object if dataType is PROTOBUF
    optional string protoClass = 4;
  }

  message Cell {
    optional uint32 columnId = 1; 
    optional bytes data = 2;
  }

  //the column info is only present for new columns in a stream of Row messages
  repeated ColumnInfo columns = 1; 
  repeated Cell cells = 2;   
}  

message ReadRowsRequest {
  // Yamcs instance name.
  optional string instance = 1;
  
  // Table name.
  optional string table = 2;
  
  // The columns to be included in the result. If unspecified, all
  // table and/or additional tuple columns will be included.
  repeated string cols = 3;
}

message WriteRowsRequest {
  // Yamcs instance name.
  optional string instance = 1;
  
  // Table name.
  optional string table = 2;

  optional Row row = 3;
}

message WriteRowsResponse {
  // The number of rows that were written
  optional uint32 count = 1;
}

message WriteRowsExceptionDetail {
  optional uint32 count = 1;
}

message ListValue {
  repeated Value values = 1;
}

message ExecuteSqlRequest {
  // Yamcs instance name.
  optional string instance = 1;
  
  // StreamSQL statement
  optional string statement = 2;
}

message ResultSet {
  repeated ColumnInfo columns = 1;
  repeated ListValue rows = 2;
}

message ListTablesRequest {
  // Yamcs instance name.
  optional string instance = 1;
}

message ListTablesResponse {
  repeated TableInfo tables = 1;
}

message GetTableRequest {
  // Yamcs instance name.
  optional string instance = 1;
  
  // Table name.
  optional string name = 2;
}

message GetTableDataRequest {
  // Yamcs instance name.
  optional string instance = 1;
  
  // Table name.
  optional string name = 2;
  
  // The columns to be included in the result. If unspecified, all table
  // and/or additional tuple columns will be included.
  repeated string cols = 3;
  
  // The zero-based row number at which to start outputting results. Default: ``0``
  // Note that in the current rocksdb storage engine there is no way to jump to a row by its number. 
  // This is why such a request will do a table scan and can be slow for large values of pos.
  optional int64 pos = 4;
  
  // The maximum number of returned records per page. Choose this value
  // too high and you risk hitting the maximum response size limit
  // enforced by the server. Default: ``100``
  optional int32 limit = 5;
  
  // The direction of the sort. Sorting is always done on the key of the
  // table. Can be either ``asc`` or ``desc``. Default: ``desc``
  optional string order = 6;
}

message ListStreamsRequest {
  // Yamcs instance name.
  optional string instance = 1;
}

message ListStreamsResponse {
  repeated StreamInfo streams = 1;
}

message GetStreamRequest {
  optional string instance = 1;
  optional string name = 2;
}

message SubscribeStreamRequest {
  optional string instance = 1;
  optional string stream = 2;
}

message ColumnData {
  optional string name = 1;
  optional Value value = 2;
}

message StreamData {
  optional string stream = 1;
  repeated ColumnData column = 2;
}

message SubscribeStreamStatisticsRequest {
  optional string instance = 1;
}

message TableData {
  message TableRecord {
    repeated ColumnData column = 1;
  }
  repeated TableRecord record = 1;
}

message ColumnInfo {
  optional string name = 1;
  optional string type = 2;
  repeated EnumValue enumValue = 3;
}

message EnumValue {
  optional int32 value = 1;
  optional string label = 2;
}

message TableInfo {
  optional string name = 1;
  repeated ColumnInfo keyColumn = 2;
  repeated ColumnInfo valueColumn = 3;
  optional string script = 4;
  repeated string histogramColumn = 5;
  optional string storageEngine = 6;
  optional int32 formatVersion = 7;
  optional string tablespace = 8;
  optional bool compressed = 9;
  optional PartitioningInfo partitioningInfo = 10;
}

message PartitioningInfo {
  enum PartitioningType {
    TIME = 1;
    VALUE = 2;
    TIME_AND_VALUE = 3;
  }
  optional PartitioningType type = 1;
  optional string timeColumn = 2;
  optional string timePartitionSchema = 3;
  optional string valueColumn = 4;
  optional string valueColumnType = 5;
}

message StreamInfo {
  optional string name = 1;
  repeated ColumnInfo column = 2;
  optional string script = 3;
  optional int64 dataCount = 4;
}


message RebuildHistogramRequest {
  // Yamcs instance name.
  optional string instance = 1;
  
  // Table name.
  optional string table = 2;
  
  // Specify a date string in ISO 8601 format. The histogram data for all partitions overlapping with the [start, stop] interval will be recreated.
  //If not specified, it is assumed as the start of the archive.
  optional google.protobuf.Timestamp start = 3;
  
  // Specify a date string in ISO 8601 format. The histogram data for all partitions overlapping with the [start, stop] interval will be recreated.
  //If not specified, it is assumed as the end of of the archive.
  optional google.protobuf.Timestamp stop = 4;
  
}

message RebuildHistogramResponse {
}